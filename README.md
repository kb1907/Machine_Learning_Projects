# Machine Learning Projects
----------------------------------------

<img src="https://miro.medium.com/max/1400/0*QYxNNYh6W9jO1b_-.png" width="550" height="450">

- In this folder, different scale Machine Learning projects can be found.
- All the projects were done by using real life data.
- This folder will be updated constantly.
- You can also see the some of the projects from my Kaggle account: https://www.kaggle.com/kaanboke
- All the best ðŸ¤˜

## Machine Learning Projects
----------------------------------------

### 1. [CatBoost-LightGBM-XGBoost-Imbalanced Data](https://www.kaggle.com/kaanboke/xgboost-lightgbm-catboost-imbalanced-data)

- Based on the customer data, I made predictions on the customer churn. Data has an imbalanced target variable, which gave me a good opportunity to look in detail at how the famous trio (XGBoost & LightGBM & Catboost) handle the imbalanced data.

<img src="https://miro.medium.com/max/1554/0*bpEDPjgIvbJOPaWz.png" width="300" height="300">, <img src="https://lightgbm.readthedocs.io/en/latest/_images/LightGBM_logo_black_text.svg" width="300" height="300">,<img src="https://avatars.mds.yandex.net/get-bunker/56833/dba868860690e7fe8b68223bb3b749ed8a36fbce/orig" width="300" height="300">

### 2. [CatBoost-LightGBM-XGBoost Explained by SHAP ðŸ¤˜](https://www.kaggle.com/kaanboke/catboost-lightgbm-xgboost-explained-by-shap)

- Based on the customer data, I made predictions on the customer churn. 
One of the main issues for the non-linear ML algorithms is the explainability. In this project, I used Shap to explain the famous trio (XGBoost & LightGBM & Catboost).
- SHAP (SHapley Additive exPlanations) is a method to explain individual predictions. SHAP is based on the game theory.

<img src="https://user-images.githubusercontent.com/51021282/148936158-d601afdc-bda9-4929-a166-e9517b0b7237.png" width="400" height="400">,<img src="https://user-images.githubusercontent.com/51021282/148942585-5792294c-f1af-4b1b-8abf-9de53bb85c28.png" width="300" height="400">,<img src="https://user-images.githubusercontent.com/51021282/148942614-f1466d6a-355f-45ad-afbd-ae3294f0b0f6.png" width="300" height="400">


### 3. [Beginner Friendly CatBoost with OPTUNA](https://www.kaggle.com/kaanboke/beginner-friendly-catboost-with-optuna)

- One of the main issues for complex algorithms, like CatBoost, is how to tune the hyperparameters efficiently. 
- In this project, I used OPTUNA to tune the hyperparameters of the CatBoost model.

<img src="https://user-images.githubusercontent.com/51021282/148937505-222108ee-4237-45fb-9182-4d9c36ffdc88.png" width="600" height="400">

### 4. [End to End ML Project- Car Price Prediction](https://www.kaggle.com/kaanboke/car-price-prediction-beginner-friendly-94-3)

- In this project, I made a beginner-friendly end-to-end ML model. 
- I applied different ML models and used RMSE(Root Mean Squared Error) as an evaluation metric.

<img src="https://user-images.githubusercontent.com/51021282/148936917-440d3d5d-e245-4666-8613-3a17221470db.png" width="600" height="400">

### 5. [A gentle intro to PyCaret](https://www.kaggle.com/kaanboke/a-gentle-intro-to-pycaret-beginner-friendly)

- As a Data Scientist & ML Engineer, most of the time I use Automated ML libraries.
- PyCaret is an open-source, low-code machine learning library in Python that automates machine learning workflows.
- PyCaret is essentially a Python wrapper around several machine learning libraries and frameworks.
- In this project I wrote a gentle intro to PyCaret library. 

<img src="https://user-images.githubusercontent.com/51021282/148938091-9ca9078f-d8e5-4984-986d-d09adadf2a88.png" width="600" height="400">


### 6. [End to End ML Project with Imbalanced Data - Stroke Prediction](https://www.kaggle.com/kaanboke/beginner-friendly-end-to-end-ml-project-enjoy)

- Most of the time, I encountered imbalanced target variables in real-life data.
- As a data scientist, it is crucial to know handling imbalanced data. 
- Also important to know which relevant evaluation metrics to use with the imbalanced data.

<img src="https://user-images.githubusercontent.com/51021282/148943859-e6d364d8-c3a2-4afa-b2c9-c3305d90223b.png" width="600" height="400">


### 7. [Customer Segmentation](https://github.com/kb1907/Machine_Learning_Projects/blob/main/ML_Projects/Customer_Segmentation.ipynb)

- In real life, we don't always have labeled data to analyze. 
- For that reason,  I focused on the unsupervised learning model in this project.
- I made a detailed customer segmentation analysis by using different cluster techniques (such as K-Means and Hierarchical Clustering).

<img src="https://www.namogoo.com/wp-content/uploads/2021/04/customer-segmentation_01.jpg" width="450" height="400">



## Machine Learning Basics
-----------------------------------------

### 1. [Feature Selection-The Most Common Methods to Know](https://www.kaggle.com/kaanboke/feature-selection-the-most-common-methods-to-know)

- In machine learning and statistics, feature selection, also known as variable selection, attribute selection, or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction.
- In this notebook, I gave detailed information about the most common feature selection methods.

![](https://miro.medium.com/max/500/0*itE5HLR57zB9qWqc.png)

### 2. [How to Prevent Data Leakage?](https://www.kaggle.com/kaanboke/how-to-prevent-the-data-leakage)

- Data leakage is the use of information in the model training process which would not be expected to be available at prediction time.
- Data leakage causes the predictive scores (metrics) to overestimate the model's utility when run in a production environment.
- How to prevent data leakage is one of the basic concepts to know.

### 3. [How to Deal with Missing Values?](https://www.kaggle.com/kaanboke/the-most-used-methods-to-deal-with-missing-values)

- Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data.

- For that reason, it is important to know the most used methods to deal with missing values.  

### 4. [The Most Common Evaluation Metrics](https://www.kaggle.com/kaanboke/the-most-common-evaluation-metrics-a-gentle-intro)
- In machine learning, evaluation metrics are used to measure the performance of machine learning models/algorithms.
- Evaluation metrics are crucial. Based on the model performance we are giving decisions.
<img src="https://user-images.githubusercontent.com/51021282/148946692-bb82bbb9-7044-4069-85d6-413a34a68664.png" width="600" height="400">

### 5. [Linear Algorithms](https://github.com/kb1907/Machine_Learning_Projects/blob/main/Machine_Learning_Basics/ml-basics-linear-algorithms.ipynb)
### 6. [Non Linear Algorithms](https://github.com/kb1907/Machine_Learning_Projects/blob/main/Machine_Learning_Basics/nonlinear-algorithms.ipynb)
### 7. [Bias Variance Tradeoff](https://github.com/kb1907/Machine_Learning_Projects/blob/main/Machine_Learning_Basics/ml-basics-bias-variance-tradeoff.ipynb)

![](https://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)
